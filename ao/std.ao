 abc plumbing conditionals blocks
 hands stdenv 
 lists loops streams process 
 xprocess transducer
 data_map
 bits
 math stats random

@doc.import.std "viable starting point for AO dictionaries"


@doc.NamingConventions
"AO makes heavy use of naming conventions. Common prefixes suggest automatic processing or type information. For example, 'doc.' indicates documentation for a word, and 'test.' indicates automatic testing in a mockup environment, and 'id.' indicates that a given expression should be equivalent to identity for all the types it accepts. Conversely, suffixes connote context. A suffix might identify a specific framework, project, spreadsheet, or programmer.
 
 Multi-part words unfortunately make plain-text AO verbose, noisy, and difficult to read. 
 
 The intention is to mitigate this issue at the editor. A set of rules (configurable per user, view, or project) shall translate common prefixes and suffixes into rendering with colors and styles and possible icons. For example, `foo.x` and `foo.y` might render the same text (just `foo`) but with different colors for the `x` vs. `y` context. Similarly, fuzzy autocomplete mechanisms in the editor can eliminate need to fully write out large words. `foo.x` might be found by typing just `fx`.
~

@doc.prefix.doc.
"AO does not have 'comments' in the usual sense. Instead, a word that starts with `doc.` can document some aspect of a dictionary - e.g. a word, project, or convention. This makes documentation a first-class, computable value, though most of the time it will simply be some text.
 
 In some cases, developers will be tempted to inject remarks directly into a word. This is doable (just `"this is a comment" drop` would do) but discouraged. Instead, consider refactoring the definition into words that can be documented independently, or that have self-explanatory names.
 
 At the moment, documentation lacks a clear convention or format, just a pseudo-markdown. Most likely, this will be addressed when we start processing documentation into HTML pages or similar.
~

@doc.prefix.test.
"Automated testing is an effective means to maintain software quality, avoid regressions, and achieve confidence in code. Test driven design can keep designs well grounded and incremental.  
 
 In AO, automatic testing is expressed simply by using the prefix 'test.' in the definitions of words. Test words can be systematically executed in a standard environment with a confined powerblock. A test can fail due to type errors, assertion failures, taking too much time or space, or emitting an error message effectfully through the powerblock.
 
 Testing has limits. You cannot prove, through testing, that bugs are absent. AO developers are encouraged also to leverage symbolic analysis, which is also expressed in a relatively dynamic manner in AO (e.g. use of the 'eqv.' or 'id.' prefixes).
~
@doc.warn.test "warnMsg --; (io) emit message"
@doc.error.test "errMsg --; (io) emit message then fail"
@warn.test "warn" p cmd.test drop
@error.test "error" p cmd.test drop
@doc.cmd.test "msg -- result; (io) call powerblock in standard location"
@cmd.test %zwl .apply x %rwz

@doc.prefix.eqv.
"There are many cases where two different expressions should be equivalent. Explicitly asserting so is useful! Not only may equivalencies serve as documentation, but they also can support automatic tests, and provide hints or tests for optimizers and refactoring tools. So prefix 'eqv' provides a generic way to suggest that two subprograms should be equivalent.
 
 The type of any 'eqv.foo' word should be:
 
         ( -- [a→b] [a→b]). 
 
 That is, developers should simply add two blocks to the current stack (in no particular order) which should have equivalent behavior modulo typeful identity. Use of the 'eqv' prefix enables external tools to systematically discover, document, and validate these assertions. Further, they may later prove useful to optimizers and code rewriting.
~

@doc.assertEQ 
"a a -- a a; (annotation) assert equivalence 
 
         {&≡} :: (a*(a*e))→(a*(a*e))
           ^ that's U+2261 (dec 8801)
 
 This asserts that the two arguments should be equivalent. As an annotation, this has no observable impact on a valid program, but it may cause some invalid programs to be rejected at compile time or fail at run time.
 
 For blocks, the notion of equivalence is a little fuzzy because behavioral equivalence is undecidable in the general case. At the very least, an implementation must pass structurally equivalent blocks (i.e. same underlying ABC code).
~
@assertEQ pw %rr {&≡} %ll wx

@doc.assertEQ1 "as `assertEQ`, then drop the top copy (common)"
@assertEQ1 assertEQ drop

@doc.assertEQ1d "x y x -- x y; assert top x equivalent to bottom x"
@assertEQ1d   swapd assertEQ1 swap
@assertEQ1dd  rotd  assertEQ1 unrot
@assertEQ1ddd rolld assertEQ1 unroll

@doc.asynch
"The `{&asynch}` annotation indicates that a block should be evaluated in parallel, allowing the rest of the computation to move on until the result is needed. This will generally be implemented in a promise pipelining style. ABC's causal commutativity feature ensures this parallelism is safe, that parallelism is constrained only by dependencies between values.
 
         {&asynch} :: (block*e)→(block*e)
         asynch    :: block -- block (on AO stack)
 
 However, a little warning: `{&asynch}` is not like threads or processes in procedural code. Asynchronous blocks will not be able to communicate with one another, and are required to terminate. We'll frequently wait for all asynchronous child computations to terminate before returning a result, or before moving on to the next transaction or paragraph.
~
@asynch %r {&asynch} %l

@doc.compile
"The `{&compile}` annotation can support explicit compilation. It operates on a block of bytecode and transparently returns a compiled version of that block, with intention to improve performance. This is potentially very useful for staged metaprogramming and other contexts where we might compose a hundred small blocks at runtime. Of course, any benefits must be weighed against the overhead for compiling and linking the code.
 
         {&compile} :: (block*e)→(block*e) 
         compile    :: block -- block (on AO stack)
 
 Static compilation is also useful, of course. Dynamic compilation can translate to static compilation via partial evaluation, or static per-word compilation may be available; see `doc.prefix.compile!` for more.
~
@compile %r {&compile} %l

@doc.static
"The `{&static}` annotation is intended to guide partial evaluation. It indicates that a value should be computable and computed at compile-time. It should help when working with embedded DSLs and staged programs.
 
         {&static} :: (v*e) → (v*e); compute v statically
         static    :: v -- v
 
 A conforming compiler will take `{&static}` as a strong suggestion, computing the argument if possible and warning the developer otherwise. Even if an argument cannot be wholly evaluated, it might be partially evaluated or lifted out of a loop. More precise annotations may be necessary in some use cases. (Ideas on how to more precisely annotate partial evaluation within the constraints of ABC are welcome.)
~
@static %r {&static} %l

@doc.assertIsolated
"The `{&isolated}` annotation declares that a block should not contain or mention any machine-specific capabilities, meaning that it does not contain any tokens other than annotations, sealers, unsealers, and dynamically linked resources that are themselves isolated. Isolation is NOT the same as purity. An isolated function may cause effects so long as it receives effectful capabilities as arguments. By design, every function in the AO language is isolated.
 
         {&isolated} :: (block * e) → (block * e)
         assertIsolated :: block -- block
 
 Isolation is not difficult to enforce dynamically or statically. Of course, as with all annotations, enforcement is strictly optional. In addition to enforcing isolation, the declaration can support optimizers by allowing them to assume isolation.
~
@assertIsolated %r {&isolated} %l

@doc.assertPure
"The `{&pure}` annotation declares that a block should neither observe or influence world state when evaluated. Similar to isolation, the block may use annotations, sealers, unsealers, and dynamically linked resources. However, purity doesn't imply the block is isolated, rather that it simply doesn't use any of the capabilities it can access.
 
         {&pure} :: (block * e) → (block * e)
         assertPure :: block -- block
 
 In the absence of a Haskell-like type system, purity is relatively difficult to statically validate compared to isolation. Purity requires a fairly precise analysis of data flow, and is frequently 'viral' by implying requirements on the arguments. That said, purity isn't especially difficult to enforce dynamically, e.g. by sandboxing a computation. In addition to enforcing purity, a purity declaration can support optimizers by allowing them to assume purity. However, if all you want is the optimizations, you should consider `assertSafe` instead of `assertPure`.
~
@assertPure %r {&pure} %l

@doc.assertSafe
"The `{&safe}` annotation declares that a block should not influence world state in any significant manner. I'm using the word 'safe' here in the same sense that W3C describes the HTTP methods GET and HEAD to be safe. Compared to purity, safety is a strictly weaker property. Safe but impure computations may observe world state. 
 
         {&safe} :: (block * e) → (block * e)
         assertSafe :: block -- block
 
 Similar to purity, safety can be difficult to validate statically, but is not especially difficult to enforce dynamically. Declaration of safety can also support optimizers. Safety allows optimizations such as dead code elimination and a restricted form of lazy evaluation (where laziness might be restricted to a particular transaction). This complements ABC's universal properties of causal commutativity and spatial idempotence.
~
@assertSafe %r {&safe} %l

@doc.lazy
"The `{&lazy}` annotation suggests that a computation involving a block should be delayed until we know the result is needed. However, it's a weak suggestion. If you really want laziness, you should model it explicitly, e.g. capturing computations with quotation and composition.
 
         {&lazy} :: (block * e) → (block * e)
         lazy :: block -- block
 
 Note that `{&lazy}` implies `{&safe}`. Annotations, including `{&lazy}`, are not permitted to affect observable behavior of programs, and safety is necessary to ensure that laziness doesn't impact observable behavior. See `assertSafe`.
~
@lazy %r {&lazy} %l


@doc.prefix.compile!
"If you define word `compile!foo` this can act as a directive that the AO system should separately compile the word `foo`. The definition of `compile!foo` should be left empty, at least for now. The `compile!` word may later provide opportunity for optimization directives.
 
 This directive leverages ABC's separate compilation and linking model. An ABC subprogram may be saved and given a unique identifier. ABC uses its effects system to load and logically inline the subprogram - e.g. `{#resourceId}`. The `compile!foo` directive tells an AO compiler to replace the word with the equivalent resource identifier and save the bytecode at an appropriate location for linking. This doesn't require compilation, but provides an excellent opportunity for it. 
 
 The actual encoding of ABC resources is more sophisticated than plain bytecode, including compression and encryption steps. Relevant pseudocode:
 
         makeResource(bytecode):
             hashBC = secureHashBC(bytecode)
             cipherText = encrypt(hashBC,compress(bytecode))
             hashCT = secureHashCT(cipherText)
             store(hashCT,cipherText)
             resourceId = encode(append(hashCT,hashBC))
             return {#resourceId}
 
 The current proposal is that secureHashBC is SHA3-384 taking the last 256 bits, and secureHashCT is SHA3-384 taking the first 128 bits. (That is, SHA3-384 is used twice, keeping independent parts for 384 bits in the end.) The encryption will likely be AES-CTR (with a zero IV). Compression is still under development, but includes a specialized compression pass for embedded binary data.
 
 The idea is that we should be able to distribute application resources through distrusted servers (cloud servers, content distribution networks, etc.), and also that we should be able to eliminate duplicates and maximize reuse via caching. There is a remaining vulnerability to 'confirmation' attacks, but developers can address this by indicating sensitive words with a `secret!foo` directive.
 
 See also: `doc.prefix.secret!` `doc.BinariesInABC`
~

@doc.prefix.secret!
"ABC's separate compilation and linking model uses [convergent encryption](http://en.wikipedia.org/wiki/Convergent_encryption), which is vulnerable to a class of 'confirmation' attacks, i.e. whereby an attacker might determine a ten digit bank account number by systematically encrypting all ten billion combinations and finding which is stored on the server. Confirmation attacks only work against low-entropy data, but a great deal of privacy-sensitive information is low entropy.
 
 To resist confirmation attacks, we can mix in a 'convergence secret', e.g. some high entropy text, with the ABC resource. If we add a 192-bit secret, the cost of a confirmation attack would increase by a factor of roughly 2^192. For compression, a 192-bit convergence secret might be encoded in ABC's base 16 as:
 
     "bpgzmjkydmfyfdhdhptzmnbpdjkndjnsgdjyzpxbsypshqfk
     ~{&secret}%
 
 A convergence secret should be deterministic, stable across compiles, and cryptographically secure. A simple HMAC of an external secret together with the protected code would work well enough. The `secret!` prefix acts as a directive to the AO compiler. If the word `secret!foo` is defined, then a convergence secret should be generated for when including code for `foo` in a resource. The directives may be used in other ways, too, such as constraining exports.
 
 In practice, most AO code will not be secret. But it might come up occasionally if using an AO dictionary more like an operating system or personal environment than a programming language.
 
 See also: `doc.BinariesInABC`
~

@TODO.AO
"Much to do about AO
  * functions for association lists, maps, records, or similar
   *   maybe support for tables, too; relational algebra
   *     ideally a DSL supporting composition, optimization, compilation
  * consider 'measured' trees, with arbitrary monoid to cache information
   * e.g. size information
  * persistent union-find
  * working with embedded binary data (alphabet a-z minus aeiou,vrwlc).
  * access to bytecodes: reflection/introspection
  * transpile ABC to JavaScript
  * functions for matrices and vector
  * look into FGL graph library (manipulating graphs with a zipper)
  * functions for sequences (finger-trees/ropes)
  * functions for parser combinators; grammar DSL
   *   should support derivative grammars & deep optimizations
  * AO bootstrap; optimizers and compilers for ABC, AO
   *   eventually do typechecking
   *   eventually get to AMBC
  * develop a colors/materials model? 
   *   named palette plus RGB? meh. 
   *   a DSL for mixing, lightening, and darkening colors?
   *   maybe a richer 'materials' model (cf. POV-ray) 
    *     with shine, roughness, transparency, etc.
    *     procedural texture generation, perlin noise
    *     automatic generation from images
    *     suitable for use in games...
  * functions for text, utf-8, binary, base64
   *   implement as stream transformers?
  * functions for kd-trees, geometries, scene-graphs
  * functions for textures, jpegs, gifs, compression and decompression
  * contraint models for staged dependency/policy/typeclass injection 
  * project euler? rosetta code? a simple tetris/breakout game?
   *   need a lot more performance, first!
  * simplified app type for quick integration with Haskell? (use plugins?)
  * 2D-3D scene-graphs based on enneatrees and zippers
  * secure pseduo-random number generators; probabilistic programs
  * math libs - linear algebra, symbolic maths
  * knowledge database or encyclopaedia?
   *   unicode
   *   world data? (countries, flags, populations, maps)
 
 AO's design philosophy is actually similar to Wolfram's - that code should have easy access to vast quantities of useful, maintainable data. AO provides this access in the form of words in a very large dictionary.
~


@doc.Zippers
"The zipper data structure was described by Gérard Huet in 1997. It enables navigation and modification of tree-structured data in a purely functional context. At any time, the original tree structure can be recovered with the modifications. For Awelon project, zippers should be widely useful for modeling user navigation through a programming environment, and also for compositional manipulation of tree structures. Zippers must generally be specialized for each data structure.
~

@doc.HigherOrderZippers
"Zippers are a first derivative on a data structure. However, higher derivatives are also very useful. For example, first derivative can focus on a single character in text, but second derivative models an expandable selection of text. In a scene graph, such could model a mobile bounding volume. I don't grasp third derivatives yet, but my intuition is that it can help structurally abstract sweeps or convolutions, e.g. the process of casting a ray through a scene-graph.
~ 

@doc.ValueSealing
"AO allows seal/unseal actions to be hard-coded using inline ABC. These are represented as:
 
         {:foo} :: (a*e) → ((foo:a)*e)  seal value with 'foo'
         {.foo} :: ((foo:a)*e) → (a*e)  unseal value with 'foo'
 
 A sealed value is like wrapping a letter in an envelope. In case of discretionary sealers like `{:foo}`, this envelope isn't very secure, but you can at least prevent accidental dependencies on the underlying data - and it's easy to search for all code that unseals a value. Aside: I've made a recent change to the structure of sealers, to now operate on a `val*env` pair and thus not requiring the full wrapper.
 
 For secure value sealing, we shall utilize symbol `$`, as follows:
 
         {:format$leftKey}              seal value cryptographically
         {.format$rightKey}             unseal cryptographic sealed value
         {$format}                      indicate cryptographically sealed value
 
 The default format, whatver is chosen, won't need to be spelled out. I would like to favor a public/private key model for asymmetric encryption, i.e. such that a holder of just the sealer cannot unseal values, and a holder of just the unsealer cannot seal values. I haven't worked out the details. But ECC does seem promising.
 
 Serializations of sealed values:
 
         #42{:foo}                      discretionary sealed value
         ["cipherText\n~c]f{$fmt}       cryptographically sealed affine value
 
 Discretionary sealers are simply serialized to bytecode as normal, then sealed again on the remote VM. The data remains accessible for introspection, reflection, and debugging. Discretionary sealers serve many useful roles such as resisting accidental data couplings, aiding a typechecker, or providing hints to a rendering or representation system.
 
 The cryptographic case is much more strongly protected. The cipher text is the result of compressing and encrypting the bytecode that will regenerate the data. We'll use the same compression as ABC resources, but we'll obviously use a different, asymmetric encryption format. The cipher text is embedded in the ABC stream as a binary (cf. `doc.BinariesInABC`). The text is further wrapped in a block so we may apply all the substructural properties relevant to the now opaque data (affine, relevant, expirations, etc.). Finally, the cryptographic text block is sealed with `{$format}` to forbid manipulations until unsealed. 
 
 Cryptographic sealers are intended mostly to support rights amplification and other security patterns. Value sealing is complementary to object capability security, and together object capabilities and value sealing can model most security policies.
 
 Sealers and unsealers of either kind may be hard-coded into the dictionary. Hard coding of secure sealers is most interesting when the other end is known by a remote service. Sealed values, however, may not be directly encoded in AO.
~

@doc.seal "a -- :a; anonymous discretionary seal; see doc.ValueSealing"
@doc.unseal ":a -- a; unseal a value sealed by 'seal'; see doc.ValueSealing"
@seal %r {:} %l
@unseal %r {.} %l

@doc.ConcurrencyInAO
"AO has a natural capacity for parallelism (see @doc.CausalCommutativity). Concurrency is related, but is a different concept from parallelism. With concurrency, we have independently specified subprograms interacting in a shared environment. 
 
 To model concurrent behavior, AO developers need only to model the shared environment and interaction - e.g. to model message queues, registrations, and so on. This sort of explicit concurrency is explored with the incremental processes model (abstractly, `µP.[a→(P*b)]`). Implicit concurrency is also feasible, but only for an effects model that respects causal commutativity and spatial idempotence, such as Reactive Demand Programming (RDP).
 
 Parallel or concurrent behavior in AO tends to be fully deterministic. Non-determinism is possible, but must be expressed explicitly. Implicit non-determinism is incompatible with spatial idempotence (see @doc.SpatialIdempotence). An [oracle machine](http://en.wikipedia.org/wiki/Oracle_machine) could feasibly observe race-conditions, i.e. modeling each race as a unique, stateful resource.
~

@doc.SpatialIdempotence
"Spatial idempotence is an assumption in AO: if a subprogram is computed many times, it must each time return the same result and have no additional side-effects beyond computing it once. This assumption can be enforced through the effects model. However, it is often enforced by affine types to simply prevent expressing of an effect more than once. 
~

@doc.CausalCommutativity
"AO assumes causal commutativity. Any two subprograms can commute if they don't have a data-dependency relationship (i.e. where the output of one subprogram computes an input to another). This assumption is naturally enforced by the ABC primitives, but must additionally be enforced by whatever effectful capabilities are granted to the AO program.
 
 Causal commutativity is a valuable basis for many optimizations, and also for parallelism. Independent subprograms can be computed in parallel. Synchronization becomes implicit when results from parallel subprograms are used together for a side-effect.
~

@doc.CompositionIsFirstPrinciple
"Every compositional model consists of a trinity: components, operators, and properties. Compositional operators are algebraically closed; they combine two components into a third component. Compositional properties are invariant or inductive over the operators, i.e. `P(x*y)=F(P(x),'*',P(y))`. To the extent the properties `P` are useful and the function `F` is simple, we can reason about the composite based on the components. A set of components may be domain specific (such as diagrams, documents, sounds, and scene-graphs) or more general purpose (such as functions, arrows, matrices, grammars).
 
 Effective use of composition often requires domain-specific tradeoffs between the three aspects. For example, by limiting what components we talk about (just diagrams) we can achieve more useful properties (e.g. bounding areas, efficient occlusion) and more operators (add, scale, rotate, translate, etc.). It is possible to switch between compositional models primarily by staging them, such that the output of one compositional model is translated to a component in another compositional model. 
 
 Composition is useful as a foundation for scalability and modularity. Compositional properties enable developers to reason about components without deep knowledge of the implementation details. Composition operators are also convenient due to their uniformity, i.e. we aren't forced to develop or learn a new ad-hoc language or interface for every object. The uniformity and compositional properties can lead to useful intuitions. In comparison, non-compositional models require an ever deeper knowledge of the dependencies to reason about behavior, and ever more ad-hoc and problem-specific glue code to integrate solutions.
 
 Many models favored in conventional programming practice - including state machines, records, process loops, nominative types, conventional conditional expressions, even parametric abstraction - are not compositional. Fortunately, compositional alternatives are available. Grammars can operate as state machines (via parsing). Datalog or relations can generalize records. Processes can be compositional if we instead model them as partial and incremental (i.e. small-step processes; `µP.a→(P*b)`). Nominative types can be replaced with structural types. If/then/else and ad-hoc pattern-case conditional expressions can be replaced with a structural sum types. Ad-hoc parametric abstraction can often be replaced with staging and a reader monad.
 
 Awelon project strongly favors compositional models and designs. This should be reflected in AO's dictionaries and ontologies. I ask that all AO developers embrace composition as first design principle, pervasively, with priority over most other features.
 
 AO is designed to serve as a relatively generic compositional layer, to enable integration or translation of domain-specific models. AO's words are themselves software components. AO's compositional properties - especially causal commutativity, spatial idempotence, substructural types, and capability security - are very useful in this role. AO is also designed to mitigate common weaknesses of deeply layered abstractions by aggressive use of partial evaluation, dependent type analysis, and program search.
~


@pangram#1 "pack my box with five dozen liquor jugs"
@pangram#2 "a quick brown fox jumped over the lazy dog"



